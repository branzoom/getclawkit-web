import requests
import json
import os
import time
from datetime import datetime
import base64
import yaml
from dotenv import load_dotenv

load_dotenv()

GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')
DRY_RUN = os.getenv('DRY_RUN', 'True').lower() == 'true'
# è°ƒè¯•æ¨¡å¼ï¼šè®¾ç½®æ•°å­— (e.g. 5) åªæŠ“å‰å‡ ä¸ªä½œè€…ç›®å½•ï¼›è®¾ç½® None æŠ“å…¨é‡
DEBUG_LIMIT = 5

OFFICIAL_REPO = "openclaw/skills"
OFFICIAL_PATH = "skills"
COMMUNITY_QUERIES = ["topic:openclaw-skill"]
MAX_DESC_LENGTH = 5000

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_DIR = os.path.join(BASE_DIR, "data")
CURRENT_FILE = os.path.join(DATA_DIR, "skills.json")
NEW_FILE = os.path.join(DATA_DIR, "skills_new.json")
BACKUP_DIR = os.path.join(DATA_DIR, "backups")

if not os.path.exists(BACKUP_DIR): os.makedirs(BACKUP_DIR)

headers = {
    "Authorization": f"token {GITHUB_TOKEN}",
    "Accept": "application/vnd.github.v3+json"
}

def fetch_file_content(url):
    try:
        r = requests.get(url, headers=headers, timeout=10)
        if r.status_code == 200:
            return base64.b64decode(r.json()['content']).decode('utf-8')
    except: pass
    return None

def parse_skill_md(content):
    if not content: return {}, ""
    if content.startswith('---'):
        try:
            parts = content.split('---', 2)
            if len(parts) >= 3:
                return yaml.safe_load(parts[1]), parts[2].strip()
        except: pass
    return {}, content

def truncate_text(text, length=MAX_DESC_LENGTH):
    if not text: return ""
    if len(text) <= length: return text
    return text[:length].rsplit(' ', 1)[0] + "\n\n...(Truncated)"

def process_official_recursive():
    print(f"\n--- Phase 1: Deep Scanning Official Repo ({OFFICIAL_REPO}) ---")
    skills_list = []
    
    # 1. èŽ·å–é¡¶å±‚ç›®å½• (Level 1: /skills)
    api_url = f"https://api.github.com/repos/{OFFICIAL_REPO}/contents/{OFFICIAL_PATH}"
    resp = requests.get(api_url, headers=headers)
    if resp.status_code != 200: return []
    
    # è¿™é‡Œæ‹¿åˆ°çš„æ˜¯ä½œè€…åˆ—è¡¨ (e.g., 0xbreadguy, 0xadamsu)
    authors = [i for i in resp.json() if i['type'] == 'dir']
    
    if DEBUG_LIMIT: 
        print(f"âš ï¸ Debug: Limiting scan to first {DEBUG_LIMIT} authors.")
        authors = authors[:DEBUG_LIMIT]

    repo_meta = requests.get(f"https://api.github.com/repos/{OFFICIAL_REPO}", headers=headers).json()
    base_stars = repo_meta.get('stargazers_count', 0)

    for idx, author_folder in enumerate(authors, 1):
        author_name = author_folder['name']
        print(f"\r[{idx}/{len(authors)}] ðŸ“‚ Scanning Author: {author_name}...", end="", flush=True)
        
        # 2. èŽ·å–ä½œè€…ç›®å½•ä¸‹çš„å­æ–‡ä»¶å¤¹ (Level 2: /skills/author/skill_name)
        author_url = author_folder['url']
        try:
            sub_resp = requests.get(author_url, headers=headers)
            if sub_resp.status_code != 200: continue
            
            # è¿™é‡Œæ‹¿åˆ°çš„æ‰æ˜¯çœŸæ­£çš„ Skills ç›®å½•
            skill_folders = [i for i in sub_resp.json() if i['type'] == 'dir']
            
            for skill_folder in skill_folders:
                skill_dir_name = skill_folder['name']
                skill_path = skill_folder['path'] # e.g. skills/0xbreadguy/megaeth-ai-developer-skills
                
                # 3. åœ¨è¿™ä¸ªç›®å½•ä¸‹æ‰¾ SKILL.md
                skill_md_url = f"https://api.github.com/repos/{OFFICIAL_REPO}/contents/{skill_path}/SKILL.md"
                readme_url = f"https://api.github.com/repos/{OFFICIAL_REPO}/contents/{skill_path}/README.md"
                
                raw_content = fetch_file_content(skill_md_url)
                if not raw_content:
                    raw_content = fetch_file_content(readme_url)
                
                # å¦‚æžœè¿ž README éƒ½æ²¡æœ‰ï¼Œå¯èƒ½ä¸æ˜¯ä¸ª Skillï¼Œè·³è¿‡
                if not raw_content: continue

                metadata, body = parse_skill_md(raw_content)
                
                # æž„é€ æ•°æ®
                # ä¼˜å…ˆç”¨ metadata é‡Œçš„ nameï¼Œæ²¡æœ‰åˆ™ç”¨æ–‡ä»¶å¤¹å
                display_name = metadata.get('name', skill_dir_name.replace('-', ' ').title())
                description = metadata.get('description', f"Official skill by {author_name}")
                
                skills_list.append({
                    "id": f"official-{author_name}-{skill_dir_name}", # IDåŒ…å«ä½œè€…åé˜²æ­¢å†²çª
                    "name": display_name,
                    "shortDesc": description,
                    "longDesc": truncate_text(body or raw_content),
                    "author": author_name, # è¿™é‡Œç”¨å­ç›®å½•åä½œä¸ºå…·ä½“ä½œè€…
                    "authorUrl": f"https://github.com/{author_name}", # å°è¯•çŒœæµ‹ä½œè€…ä¸»é¡µï¼Œæˆ–è€…æŒ‡å‘ openclaw
                    "category": "Official",
                    "downloads": base_stars,
                    "stars": base_stars,
                    "lastUpdated": datetime.now().strftime('%Y-%m-%d'),
                    "version": metadata.get('version', '1.0.0'),
                    "license": "MIT",
                    "command": f"clawhub install {OFFICIAL_REPO}/{skill_path}",
                    "downloadUrl": skill_folder['html_url'],
                    "safetyRating": "Verified",
                    "tags": ["official"] + metadata.get('tags', [])
                })
                # ç¨å¾®å»¶æ—¶é˜²æ­¢é€ŸçŽ‡é™åˆ¶
                time.sleep(0.05)
                
        except Exception as e:
            print(f" Error: {e}")
            pass

    return skills_list

def process_community():
    # ç¤¾åŒºéƒ¨åˆ†é€»è¾‘ä¿æŒä¸å˜ (ç•¥)ï¼Œå› ä¸ºå®ƒæœ¬æ¥å°±æ˜¯å•å±‚ç»“æž„ï¼Œä¸éœ€è¦æ”¹
    print(f"\n\n--- Phase 2: Scanning Community ---")
    unique_repos = {}
    for q in COMMUNITY_QUERIES:
        params = {"q": q, "sort": "stars", "order": "desc"}
        try:
            r = requests.get("https://api.github.com/search/repositories", headers=headers, params=params)
            if r.status_code == 200:
                for item in r.json().get('items', []):
                    if item['full_name'] != OFFICIAL_REPO: unique_repos[item['id']] = item
        except: pass

    all_items = list(unique_repos.values())
    skills_list = []
    
    for idx, item in enumerate(all_items, 1):
        try:
            repo_name = item['name']
            owner = item['owner']['login']
            branch = item.get('default_branch', 'main')
            print(f"\r[{idx}/{len(all_items)}] ðŸŒ Community: {repo_name}...", end="", flush=True)
            
            # å°è¯•èŽ·å– SKILL.md
            skill_md_url = f"https://api.github.com/repos/{owner}/{repo_name}/contents/SKILL.md"
            readme_url = f"https://api.github.com/repos/{owner}/{repo_name}/contents/README.md"
            
            raw_content = fetch_file_content(skill_md_url) or fetch_file_content(readme_url)
            metadata, body = parse_skill_md(raw_content)
            
            skills_list.append({
                "id": repo_name.lower(),
                "name": metadata.get('name', repo_name.replace('-', ' ').title()),
                "shortDesc": metadata.get('description', item['description'] or f"Community skill"),
                "longDesc": truncate_text(body or raw_content),
                "author": owner,
                "authorUrl": item['owner']['html_url'],
                "category": "Community",
                "downloads": item.get('forks_count', 0),
                "stars": item.get('stargazers_count', 0),
                "lastUpdated": item['updated_at'].split('T')[0],
                "version": metadata.get('version', 'latest'),
                "license": item['license']['spdx_id'] or "None",
                "command": f"clawhub install {item['html_url']}",
                "downloadUrl": f"https://github.com/{owner}/{repo_name}/archive/refs/heads/{branch}.zip",
                "safetyRating": "Community",
                "tags": metadata.get('tags', item.get('topics', []))
            })
        except: pass
        
    return skills_list

def main():
    print(f"--- ðŸš€ Deep Skill Hunt (Dry Run: {DRY_RUN}) ---")
    if not GITHUB_TOKEN: return print("âŒ Error: GITHUB_TOKEN missing.")

    # è¿™é‡Œè°ƒç”¨æ–°çš„é€’å½’å‡½æ•°
    skills = process_official_recursive() + process_community()
    skills.sort(key=lambda x: x['stars'], reverse=True)
    
    if not skills: return print("\nâŒ No skills found.")

    print(f"\n\nðŸ’¾ Saving {len(skills)} skills to: {os.path.basename(NEW_FILE)}")
    with open(NEW_FILE, 'w', encoding='utf-8') as f:
        json.dump(skills, f, indent=2, ensure_ascii=False)

    if not DRY_RUN:
        print("\nðŸ”„ Updating LIVE files...")
        if os.path.exists(CURRENT_FILE):
            bk = f"skills_bk_{datetime.now().strftime('%H%M%S')}.json"
            os.rename(CURRENT_FILE, os.path.join(BACKUP_DIR, bk))
        os.replace(NEW_FILE, CURRENT_FILE)
        print("   âœ… DONE!")
    else:
        print("\nâš ï¸ DRY RUN: Live file NOT updated.")

if __name__ == "__main__":
    main()